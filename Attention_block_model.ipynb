{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PE-fuDNlpNZg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f0255a8-c266-4a53-c575-0ab249404e83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import *\n",
        "import h5py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the seed for reproducibility\n",
        "np.random.seed(66)\n",
        "tf.random.set_seed(66)\n",
        "\n",
        "base_path = '/content/drive/MyDrive/CS_638_Term_project_data/'\n",
        "train_x_path = os.path.join(base_path, 'camelyonpatch_level_2_split_train_x.h5')\n",
        "train_y_path = os.path.join(base_path, 'camelyonpatch_level_2_split_train_y.h5')\n",
        "valid_x_path = os.path.join(base_path, 'camelyonpatch_level_2_split_valid_x.h5')\n",
        "valid_y_path = os.path.join(base_path, 'camelyonpatch_level_2_split_valid_y.h5')\n",
        "test_x_path = os.path.join(base_path, 'camelyonpatch_level_2_split_test_x.h5')\n",
        "test_y_path = os.path.join(base_path, 'camelyonpatch_level_2_split_test_y.h5')\n"
      ],
      "metadata": {
        "id": "LChRakzlpX-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "\n",
        "def normalize_images(images):\n",
        "    \"\"\"Preprocess image pixel values for ResNet50.\"\"\"\n",
        "    # Convert to float32 for precision\n",
        "    images = images.astype('float32')\n",
        "    # Use the preprocess_input function specific to the model\n",
        "    images = preprocess_input(images)\n",
        "    return images"
      ],
      "metadata": {
        "id": "SYCcj33DpdEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define the data augmentation\n",
        "data_augmentation = ImageDataGenerator(\n",
        "    preprocessing_function=normalize_images,  # Assuming you want to keep using your normalize_images function\n",
        "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "    horizontal_flip=True,  # randomly flip images\n",
        "    vertical_flip=True  # randomly flip images\n",
        ")"
      ],
      "metadata": {
        "id": "j8NgtEWlpw38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hdf5_image_feature_generator(x_path, y_path, feature_array, batch_size, shuffle=False, augmentor=None):\n",
        "    while True:\n",
        "        with h5py.File(x_path, 'r') as x_h5, h5py.File(y_path, 'r') as y_h5:\n",
        "            x = x_h5['x']\n",
        "            y = y_h5['y']\n",
        "            num_samples = x.shape[0]\n",
        "\n",
        "            indices = np.arange(num_samples)\n",
        "            if shuffle:\n",
        "                np.random.shuffle(indices)\n",
        "\n",
        "            for start_idx in range(0, num_samples, batch_size):\n",
        "                end_idx = min(start_idx + batch_size, num_samples)\n",
        "                batch_indices = indices[start_idx:end_idx]\n",
        "\n",
        "                batch_x = x[batch_indices]\n",
        "\n",
        "                if augmentor is not None:\n",
        "                    # Apply augmentation here\n",
        "                    for i in range(batch_x.shape[0]):\n",
        "                        batch_x[i] = augmentor.random_transform(batch_x[i])\n",
        "\n",
        "                batch_y = np.array(y[batch_indices]).astype('float32')\n",
        "                batch_features = feature_array[batch_indices]\n",
        "\n",
        "                # Ensure that batch_y is 2D and batch_features has the correct shape\n",
        "                batch_y = np.squeeze(batch_y)\n",
        "                assert len(batch_y.shape) == 1, \"Labels should be 1D\"\n",
        "                assert batch_features.shape[0] == batch_x.shape[0], \"Features batch size does not match images batch size\"\n",
        "\n",
        "                yield [batch_x, batch_features], batch_y"
      ],
      "metadata": {
        "id": "Nsxw2Bw9p2tD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(images):\n",
        "    # Calculate the mean and standard deviation of pixel values\n",
        "    pixel_mean = images.mean(axis=(1, 2, 3))\n",
        "    pixel_std = images.std(axis=(1, 2, 3))\n",
        "    pixel_max = images.max(axis=(1, 2, 3))\n",
        "    pixel_min = images.min(axis=(1, 2, 3))\n",
        "\n",
        "    # More features can be added here, such as skewness, kurtosis, etc.\n",
        "\n",
        "    # Combine features into a single array\n",
        "    features = np.stack([pixel_mean, pixel_std, pixel_max, pixel_min], axis=1)\n",
        "    return features"
      ],
      "metadata": {
        "id": "YE6pdLQop5Dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features_in_chunks(hdf5_path, batch_size=1000):\n",
        "    # Initialize an empty list to store the extracted features\n",
        "    all_features = []\n",
        "\n",
        "    with h5py.File(hdf5_path, 'r') as hdf5_file:\n",
        "        # Determine the number of images\n",
        "        num_images = hdf5_file['x'].shape[0]\n",
        "\n",
        "        # Process the dataset in batches\n",
        "        for start_idx in range(0, num_images, batch_size):\n",
        "            end_idx = min(start_idx + batch_size, num_images)\n",
        "            images_batch = hdf5_file['x'][start_idx:end_idx]\n",
        "\n",
        "            # Extract features for the current batch\n",
        "            batch_features = extract_features(images_batch)\n",
        "            all_features.append(batch_features)\n",
        "\n",
        "    # Concatenate all batch features together\n",
        "    all_features = np.concatenate(all_features, axis=0)\n",
        "    return all_features\n",
        "\n",
        "# Call the function to extract features for the entire dataset\n",
        "# Assuming 'train_x_path' is the path to your HDF5 file containing the images\n",
        "# Extract features for the training dataset\n",
        "train_features = extract_features_in_chunks(train_x_path)\n",
        "\n",
        "# Extract features for the validation dataset\n",
        "valid_features = extract_features_in_chunks(valid_x_path)\n",
        "\n",
        "# Calculate mean and std from training features\n",
        "mean = train_features.mean(axis=0)\n",
        "std = train_features.std(axis=0)\n",
        "\n",
        "# Apply standardization using training data statistics\n",
        "train_features = (train_features - mean) / std\n",
        "valid_features = (valid_features - mean) / std\n",
        "\n",
        "# Verify the sizes\n",
        "print(f\"Training features shape: {train_features.shape}\")\n",
        "print(f\"Validation features shape: {valid_features.shape}\")\n",
        "\n",
        "\n",
        "# Now 'extracted_features' should have as many rows as there are images in your dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "ZPcQ1YH_p9iK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15bd9f99-b43b-4972-d4b4-ecccff431c89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training features shape: (262144, 4)\n",
            "Validation features shape: (32768, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Create the augmented training data generator\n",
        "train_augmented_generator = hdf5_image_feature_generator(\n",
        "    train_x_path, train_y_path, train_features, batch_size, shuffle=False, augmentor=data_augmentation)\n",
        "\n",
        "# Create a non-augmented validation data generator\n",
        "validation_generator = hdf5_image_feature_generator(\n",
        "    valid_x_path, valid_y_path, valid_features, batch_size, shuffle=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "zIdBq18Op_AE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Dropout, Flatten, Multiply, Reshape, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "\n",
        "def attention_block(inputs):\n",
        "     # Perform global average pooling on the inputs to get a vector\n",
        "     gap = GlobalAveragePooling2D()(inputs)  # Shape: (batch_size, channels)\n",
        "\n",
        "     # Learn a dense layer to predict attention scores from the GAP vector\n",
        "     attention_scores = Dense(inputs.shape[-1], activation='softmax', use_bias=False)(gap)  # Shape: (batch_size, channels)\n",
        "\n",
        "     # Apply attention scores to the original inputs\n",
        "     attention_scores = Reshape((1, 1, inputs.shape[-1]))(attention_scores)  # Shape: (batch_size, 1, 1, channels)\n",
        "     attention_output = Multiply()([inputs, attention_scores])\n",
        "\n",
        "     return attention_output\n",
        "\n",
        "\n",
        " # Modified model building function that includes an additional input for the extracted features\n",
        "def build_model(num_features):\n",
        "     # Input for the image data\n",
        "     image_input = Input(shape=(96, 96, 3))\n",
        "\n",
        "     # Convolutional layers\n",
        "     conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(image_input)\n",
        "     conv1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "     conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv1)\n",
        "     conv2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "     conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv2)\n",
        "     conv3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "\n",
        "     # Apply the attention mechanism\n",
        "     attention = attention_block(conv3)\n",
        "     flatten_images = Flatten()(attention)\n",
        "\n",
        "     # Input for the extracted features\n",
        "     features_input = Input(shape=(num_features,))\n",
        "\n",
        "     # Concatenate the image features and the extracted features\n",
        "     concatenated_features = Concatenate()([flatten_images, features_input])\n",
        "\n",
        "     # Dense layers\n",
        "     dense1 = Dense(64, activation='relu')(concatenated_features)\n",
        "     dropout = Dropout(0.5)(dense1)\n",
        "     output = Dense(1, activation='sigmoid')(dropout)\n",
        "\n",
        "     # Build the model\n",
        "     model = Model(inputs=[image_input, features_input], outputs=output)\n",
        "\n",
        "     # Compile the model\n",
        "     opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "     model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy', tf.keras.metrics.AUC()])\n",
        "\n",
        "     return model"
      ],
      "metadata": {
        "id": "dZJLAiaRqBn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_features = train_features.shape[1]  # This should be the number of engineered features per sample\n",
        "model = build_model(num_features)\n",
        "\n",
        "\n",
        "# Estimate steps per epoch for training and validation\n",
        "with h5py.File(train_x_path, 'r') as f:\n",
        "    train_steps_per_epoch = f['x'].shape[0] // batch_size\n",
        "\n",
        "with h5py.File(valid_x_path, 'r') as f:\n",
        "    validation_steps = f['x'].shape[0] // batch_size"
      ],
      "metadata": {
        "id": "Lbhvow5oqdH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "# Define the callback to reduce the learning rate when a plateau is reached\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
        "\n",
        "# Include this callback in the fit function\n",
        "history = model.fit(\n",
        "    train_augmented_generator,\n",
        "    steps_per_epoch=train_steps_per_epoch,\n",
        "    epochs=5,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_steps,\n",
        "    callbacks=[reduce_lr]\n",
        ")"
      ],
      "metadata": {
        "id": "OhSCtyMJrDK9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bce24913-eaf5-4c89-cb93-fa2bb5aaa5e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "8192/8192 [==============================] - 799s 96ms/step - loss: 0.5264 - accuracy: 0.7474 - auc: 0.8206 - val_loss: 0.5233 - val_accuracy: 0.7246 - val_auc: 0.8194 - lr: 0.0010\n",
            "Epoch 2/5\n",
            "8192/8192 [==============================] - 760s 93ms/step - loss: 0.5025 - accuracy: 0.7610 - auc: 0.8402 - val_loss: 0.5223 - val_accuracy: 0.7273 - val_auc: 0.8207 - lr: 0.0010\n",
            "Epoch 3/5\n",
            "8192/8192 [==============================] - 768s 94ms/step - loss: 0.4993 - accuracy: 0.7627 - auc: 0.8428 - val_loss: 0.5178 - val_accuracy: 0.7279 - val_auc: 0.8232 - lr: 0.0010\n",
            "Epoch 4/5\n",
            "3821/8192 [============>.................] - ETA: 6:36 - loss: 0.4990 - accuracy: 0.7630 - auc: 0.8432"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path for saving the model\n",
        "model_save_path = '/content/drive/MyDrive/CS_638_Term_project_data/my_model_attention.h5'\n",
        "\n",
        "# Save the model\n",
        "model.save(model_save_path)\n",
        "\n",
        "print(f'Model saved to {model_save_path}')\n"
      ],
      "metadata": {
        "id": "Qd14f9XCrKGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model_save_path = '/content/drive/MyDrive/CS_638_Term_project_data/my_model_attention.h5'\n",
        "# Load the model\n",
        "loaded_model = load_model(model_save_path)\n"
      ],
      "metadata": {
        "id": "nyJpx1LtwHif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_features = extract_features_in_chunks(test_x_path)\n",
        "\n",
        "test_features = (test_features - mean) / std\n",
        "\n",
        "test_generator = hdf5_image_feature_generator(test_x_path, test_y_path, test_features,\n",
        "                                              batch_size, shuffle=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "wArNVLyMx7Ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy, test_auc = loaded_model.evaluate(test_generator, steps = np.ceil(test_features.shape[0]/batch_size))\n",
        "\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "print(f\"Test AUC: {test_auc}\")"
      ],
      "metadata": {
        "id": "nEae_9iJEfE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot loss vs epoch\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss vs. Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot accuracy vs epoch\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Accuracy vs. Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AwKmL8BLFP6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions\n",
        "predictions = loaded_model.predict(test_generator, steps=np.ceil(test_features.shape[0] / batch_size))\n",
        "\n",
        "# Ensure predicted_classes is a 1D array\n",
        "predicted_classes = (predictions > 0.5).astype(\"int32\").flatten()\n",
        "\n",
        "# Load actual labels and ensure it is a 1D array\n",
        "with h5py.File(test_y_path, 'r') as f:\n",
        "    actual_classes = np.array(f['y']).flatten()\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(actual_classes, predicted_classes)\n",
        "\n",
        "# Display confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Class 0', 'Class 1'])\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "8kZTAJmZFVw5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}