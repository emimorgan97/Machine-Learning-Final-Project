{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emimorgan97/Machine-Learning-Final-Project/blob/main/NMF_full_image.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5f6icSFGxIzg"
      },
      "outputs": [],
      "source": [
        "import gzip\n",
        "import h5py\n",
        "import numpy as np\n",
        "import shutil\n",
        "import zipfile\n",
        "from io import BytesIO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgJMKWg8w4-D",
        "outputId": "8c9ae7ce-68cc-46e6-f5b1-ccec41f3dc20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70OWqp0CrWPk"
      },
      "outputs": [],
      "source": [
        "'''import numpy as np\n",
        "import h5py\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import gzip\n",
        "\n",
        "# Function to read a batch of images from HDF5 file\n",
        "def read_images(images_file_path, labels_file_path, class_index, batch_size):\n",
        "    with gzip.open(images_file_path, 'rb') as f_images:\n",
        "        with h5py.File(f_images, 'r') as hf_images:\n",
        "            with gzip.open(labels_file_path, 'rb') as f_labels:\n",
        "                with h5py.File(f_labels, 'r') as hf_labels:\n",
        "                    images = hf_images['x']\n",
        "                    labels = hf_labels['y']\n",
        "                    class_indices = np.where(labels[:] == class_index)[0]\n",
        "                    batch_indices = class_indices[:batch_size]\n",
        "                    batch_data = images[batch_indices]\n",
        "    return batch_data\n",
        "\n",
        "# Load batch of 200 images of class 0\n",
        "images_h5_gz_file_path = '/content/drive/MyDrive/ML_Data/pcamv1/camelyonpatch_level_2_split_train_x.h5.gz'\n",
        "labels_h5_gz_file_path = '/content/drive/MyDrive/ML_Data/pcamv1/camelyonpatch_level_2_split_train_y.h5.gz'\n",
        "class_index = 0\n",
        "batch_size = 200\n",
        "batch_data = read_images(images_h5_gz_file_path, labels_h5_gz_file_path, class_index, batch_size)\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snioRV5W-64n"
      },
      "outputs": [],
      "source": [
        "'''# Standardize the data\n",
        "scaler = MinMaxScaler()\n",
        "batch_data_scaled = scaler.fit_transform(batch_data.reshape(batch_size, -1))\n",
        "\n",
        "# Initialize lists to store explained variance\n",
        "explained_variance = []\n",
        "\n",
        "# Define a range of number of components to try\n",
        "n_components_range = range(20, 100)  # Adjust range as needed\n",
        "\n",
        "# Iterate over the number of components\n",
        "for n_components in n_components_range:\n",
        "    # Create NMF model\n",
        "    print(n_components)\n",
        "    nmf = NMF(n_components=n_components, init='nndsvd', max_iter=250, tol=2e-2)\n",
        "\n",
        "    # Fit NMF model to the data\n",
        "    nmf.fit(batch_data_scaled)\n",
        "\n",
        "    # Calculate the explained variance ratio manually\n",
        "    reconstructed_data = np.dot(nmf.transform(batch_data_scaled), nmf.components_)\n",
        "    explained_variance.append(1 - np.sum((batch_data_scaled - reconstructed_data) ** 2) / np.sum(batch_data_scaled ** 2))\n",
        "\n",
        "# Plot the elbow curve\n",
        "plt.plot(n_components_range, explained_variance, marker='o')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Explained Variance')\n",
        "plt.title('Elbow Curve for NMF')\n",
        "plt.show()\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4NpQUKH_Cvz"
      },
      "outputs": [],
      "source": [
        "'''#for class 1\n",
        "\n",
        "# Load batch of 200 images of class 0\n",
        "images_h5_gz_file_path = '/content/drive/MyDrive/ML_Data/pcamv1/camelyonpatch_level_2_split_train_x.h5.gz'\n",
        "labels_h5_gz_file_path = '/content/drive/MyDrive/ML_Data/pcamv1/camelyonpatch_level_2_split_train_y.h5.gz'\n",
        "class_index = 1\n",
        "batch_size = 200\n",
        "batch_data = read_images(images_h5_gz_file_path, labels_h5_gz_file_path, class_index, batch_size)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "batch_data_scaled = scaler.fit_transform(batch_data.reshape(batch_size, -1))\n",
        "\n",
        "# Initialize lists to store explained variance\n",
        "explained_variance = []\n",
        "\n",
        "# Define a range of number of components to try\n",
        "n_components_range = range(1, 21)  # Adjust range as needed\n",
        "\n",
        "# Iterate over the number of components\n",
        "for n_components in n_components_range:\n",
        "    # Create NMF model\n",
        "    nmf = NMF(n_components=n_components, init='nndsvd', tol=2e-2)\n",
        "\n",
        "    # Fit NMF model to the data\n",
        "    nmf.fit(batch_data_scaled)\n",
        "\n",
        "    # Calculate explained variance and append to the list\n",
        "    explained_variance.append(nmf.explained_variance_ratio_.sum())\n",
        "\n",
        "# Plot the elbow curve\n",
        "plt.plot(n_components_range, explained_variance, marker='o')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Explained Variance')\n",
        "plt.title('Elbow Curve for NMF')\n",
        "plt.show()\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "import gzip\n",
        "import shutil\n",
        "from sklearn.decomposition import NMF\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "\n",
        "# Function to apply NMF to the data\n",
        "def apply_nmf_and_save(images_file_path, labels_file_path, output_file_path, batch_size, n_components, class_index, max_batches=None):\n",
        "    nmf = NMF(n_components=n_components, init='random', tol=2e-2)\n",
        "    with h5py.File(output_file_path, 'w') as hf_out:\n",
        "        nmf_group = hf_out.create_group('nmf_data')\n",
        "\n",
        "        with gzip.open(images_file_path, 'rb') as f:\n",
        "            with h5py.File(f, 'r') as hf_images:\n",
        "                with gzip.open(labels_file_path, 'rb') as f_labels:\n",
        "                    with h5py.File(f_labels, 'r') as hf_labels:\n",
        "                        images = hf_images['x']\n",
        "                        labels = hf_labels['y']\n",
        "\n",
        "                        class_indices = np.where(labels[:] == class_index)[0]\n",
        "                        num_samples = len(class_indices)\n",
        "\n",
        "                        # Calculate the number of batches to process\n",
        "                        if max_batches:\n",
        "                            num_batches = min(num_samples // batch_size, max_batches)\n",
        "                        else:\n",
        "                            num_batches = num_samples // batch_size\n",
        "\n",
        "                        # Process each batch\n",
        "                        for batch_num in range(num_batches):\n",
        "                            print(batch_num)\n",
        "\n",
        "                            start_idx = batch_num * batch_size\n",
        "                            end_idx = min(start_idx + batch_size, num_samples)\n",
        "                            batch_indices = class_indices[start_idx:end_idx]\n",
        "                            batch_data = images[batch_indices]\n",
        "\n",
        "                            # Flatten the batch data\n",
        "                            batch_data_flattened = batch_data.reshape(batch_data.shape[0], -1)\n",
        "\n",
        "                            # Apply NMF to the batch data\n",
        "                            W = nmf.fit_transform(batch_data_flattened)\n",
        "                            H = nmf.components_\n",
        "                            nmf_data = np.dot(W, H)\n",
        "\n",
        "                            # Concatenate original image data with NMF-transformed data\n",
        "                            concatenated_data = np.concatenate((batch_data_flattened, nmf_data), axis=1)\n",
        "\n",
        "                            # Store concatenated data in output .h5 file\n",
        "                            nmf_group.create_dataset(f'batch_{start_idx}', data=concatenated_data)\n",
        "\n",
        "                            # Stop if max_batches is reached\n",
        "                            if max_batches and batch_num + 1 >= max_batches:\n",
        "                                break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmaEghmduUUo",
        "outputId": "4b627168-8187-4a19-a425-7b177ac2759e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:pydrive is deprecated and no longer maintained. We recommend that you migrate your projects to pydrive2, the maintained fork of pydrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0dZ9o1kLo5H"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "images_h5_gz_file_path = '/content/drive/MyDrive/ML_Data/pcamv1/camelyonpatch_level_2_split_train_x.h5.gz'\n",
        "labels_h5_gz_file_path = '/content/drive/MyDrive/ML_Data/pcamv1/camelyonpatch_level_2_split_train_y.h5.gz'\n",
        "output_h5_file_path = 'output_data_class0_nmf.h5'\n",
        "gzipped_file_path = 'output_data_class0_nmf.h5.gz'\n",
        "batch_size = 200\n",
        "n_components = 30  # Number of components for NMF\n",
        "class_index = 0  # Specify the index of the class you want to extract (e.g., 0 for the first class)\n",
        "\n",
        "max_batches = 150  # Limit to 30,000 images\n",
        "apply_nmf_and_save(images_h5_gz_file_path, labels_h5_gz_file_path, output_h5_file_path, batch_size, n_components, class_index, max_batches=max_batches)\n",
        "\n",
        "# Compress the .h5 file\n",
        "with open(output_h5_file_path, 'rb') as f_in:\n",
        "    with gzip.open(gzipped_file_path, 'wb') as f_out:\n",
        "        shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "# Move the gzipped file to Google Drive\n",
        "destination_drive_path = '/content/drive/MyDrive/'\n",
        "shutil.copy(gzipped_file_path, destination_drive_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wp8M72J4OM08",
        "outputId": "b9419202-e691-412a-a472-119e829f2442"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n"
          ]
        }
      ],
      "source": [
        "images_h5_gz_file_path = '/content/drive/MyDrive/ML_Data/pcamv1/camelyonpatch_level_2_split_train_x.h5.gz'\n",
        "labels_h5_gz_file_path = '/content/drive/MyDrive/ML_Data/pcamv1/camelyonpatch_level_2_split_train_y.h5.gz'\n",
        "output_h5_file_path = 'output_data_class1_nmf.h5'\n",
        "gzipped_file_path = 'output_data_class1_nmf.h5.gz'\n",
        "batch_size = 200\n",
        "n_components = 30\n",
        "class_index = 1  # Specify the index of the class you want to extract (e.g., 0 for the first class)\n",
        "\n",
        "max_batches = 150 # Limit to 30,000 images\n",
        "apply_nmf_and_save(images_h5_gz_file_path, labels_h5_gz_file_path, output_h5_file_path, batch_size, n_components, class_index, max_batches=max_batches)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compress the .h5 file\n",
        "with open(output_h5_file_path, 'rb') as f_in:\n",
        "    with gzip.open(gzipped_file_path, 'wb') as f_out:\n",
        "        shutil.copyfileobj(f_in, f_out)"
      ],
      "metadata": {
        "id": "ugM20STP2A-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Ue269u6GWwR3",
        "outputId": "f9581c9b-2be0-44ca-9fbf-aa1e5413932d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/output_data_class1_nmf.h5.gz'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Move the gzipped file to Google Drive\n",
        "destination_drive_path = '/content/drive/MyDrive'\n",
        "shutil.copy(gzipped_file_path, destination_drive_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPNcsYzINiTYaaYsYG91i/8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}